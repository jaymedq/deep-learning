{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaymedq/deep-learning/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjz1b0ikWGZl"
      },
      "source": [
        "# **Ex. 1 - First steps with scikit-learn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cguXxD5AWGZl"
      },
      "source": [
        "Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv6qqG6hWGZl",
        "outputId": "d5d3956d-3168-4b5f-970f-7404f86ac007"
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, [2, 3]]\n",
        "y = iris.target\n",
        "\n",
        "print('Class labels:', np.unique(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class labels: [0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiguxh6uWGZm"
      },
      "source": [
        "Splitting data into 70% training and 30% test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_hj2-zIWGZm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=1, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guN3SZdpWGZm",
        "outputId": "e90f8610-296a-4bf6-cf63-71896abcc6b4"
      },
      "source": [
        "print('Labels counts in y:', np.bincount(y))\n",
        "print('Labels counts in y_train:', np.bincount(y_train))\n",
        "print('Labels counts in y_test:', np.bincount(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels counts in y: [50 50 50]\n",
            "Labels counts in y_train: [35 35 35]\n",
            "Labels counts in y_test: [15 15 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40r0gTCWGZn"
      },
      "source": [
        "Standardizing the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zSVqlcWWGZn"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9mmLYejQviq"
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
        "\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0],\n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.8,\n",
        "                    c=colors[idx],\n",
        "                    marker=markers[idx],\n",
        "                    label=cl,\n",
        "                    edgecolor='black')\n",
        "\n",
        "    # highlight test samples\n",
        "    if test_idx:\n",
        "        # plot all samples\n",
        "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
        "\n",
        "        plt.scatter(X_test[:, 0],\n",
        "                    X_test[:, 1],\n",
        "                    c='',\n",
        "                    edgecolor='black',\n",
        "                    alpha=1.0,\n",
        "                    linewidth=1,\n",
        "                    marker='o',\n",
        "                    s=100,\n",
        "                    label='test set')\n",
        "\n",
        "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
        "y_combined = np.hstack((y_train, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ljvbYDw-WGZr"
      },
      "source": [
        "# **Ex. 2 - Modeling class probabilities via logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7g7oez5WGZv"
      },
      "source": [
        "### *Part 1 - Train a logistic regression model* (check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with scikit-learn: use C=100.0 and random_state=1. Plot the decision regions (use plot_decision_regions from mlxtend). Evaluate the results on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwSRFpafWGZy"
      },
      "source": [
        "### *Part 2 - Tackling overfitting via regularization*: evaluate different values for C (up to 100) and observe the accuracy difference between training and test sets. Plot the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rv2eHUfWGZz"
      },
      "source": [
        "# **Ex. 3 - Maximum margin classification with support vector machines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8iIeSx9WGZ0"
      },
      "source": [
        "### *Part 1 - Dealing with the nonlinearly separable case using slack variables* (check linear kernel at https://scikit-learn.org/stable/modules/svm.html): use C=1.0 and random_state=1. Plot the decision regions (use plot_decision_regions from mlxtend and included above). Evaluate the results on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUPJzVaNWGZ3"
      },
      "source": [
        "### *Part 2 - Using the kernel trick to find separating hyperplanes in higher dimensional space*: use kernel='rbf', C=1.0, gamma=0.2, and random_state=1. Plot the decision regions (use plot_decision_regions from mlxtend and included above). Evaluate the results on the test set. Compare and discuss the results with the previous case. Try different values for the kernel parameter and evaluate the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OORaireHWGZ4"
      },
      "source": [
        "# **Ex. 4 - Decision tree learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5lOMXIpWGZ6"
      },
      "source": [
        "### *Part 1 - Building a decision tree* (check https://scikit-learn.org/stable/modules/tree.html): use criterion='entropy', max_depth=4, random_state=1. Plot the decision regions (use plot_decision_regions from mlxtend). Evaluate the results on the test set. Try different values for the max_depth and check the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSH8wZFdWGZ8"
      },
      "source": [
        "### *Part 2 - Combining weak to strong learners via random forests* (check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html): use criterion='entropy', n_estimators=25, max_depth=4, random_state=1. Plot the decision regions (use plot_decision_regions from mlxtend). Evaluate the results on the test set. Compare with the single tree case. Try different values for the number of trees and check the result."
      ]
    }
  ]
}
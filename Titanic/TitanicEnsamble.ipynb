{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python39\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 2ms/step - loss: 0.6893 - accuracy: 0.5421\n",
      "Epoch 2/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.6731\n",
      "Epoch 3/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.6606\n",
      "Epoch 4/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.7677\n",
      "Epoch 5/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.7585\n",
      "Epoch 6/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.7688\n",
      "Epoch 7/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7779\n",
      "Epoch 8/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.7711\n",
      "Epoch 9/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7756\n",
      "Epoch 10/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7882\n",
      "Epoch 11/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7779\n",
      "Epoch 12/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.7756\n",
      "Epoch 13/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.7790\n",
      "Epoch 14/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7859\n",
      "Epoch 15/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7790\n",
      "Epoch 16/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7961\n",
      "Epoch 17/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.7836\n",
      "Epoch 18/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.7995\n",
      "Epoch 19/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7870\n",
      "Epoch 20/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.7859\n",
      "Epoch 21/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4819 - accuracy: 0.7790\n",
      "Epoch 22/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7790\n",
      "Epoch 23/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.7961\n",
      "Epoch 24/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7870\n",
      "Epoch 25/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7961\n",
      "Epoch 26/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7802\n",
      "Epoch 27/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7961\n",
      "Epoch 28/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.7893\n",
      "Epoch 29/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.7882\n",
      "Epoch 30/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7950\n",
      "Epoch 31/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.7825\n",
      "Epoch 32/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7927\n",
      "Epoch 33/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.7893\n",
      "Epoch 34/60\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4521 - accuracy: 0.7882\n",
      "Epoch 35/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7859\n",
      "Epoch 36/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4473 - accuracy: 0.7938\n",
      "Epoch 37/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7938\n",
      "Epoch 38/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7950\n",
      "Epoch 39/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7870\n",
      "Epoch 40/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.7904\n",
      "Epoch 41/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7961\n",
      "Epoch 42/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.7961\n",
      "Epoch 43/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.7938\n",
      "Epoch 44/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.7950\n",
      "Epoch 45/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.7870\n",
      "Epoch 46/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.7927\n",
      "Epoch 47/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.7882\n",
      "Epoch 48/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.8007\n",
      "Epoch 49/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.7938\n",
      "Epoch 50/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7870\n",
      "Epoch 51/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7859\n",
      "Epoch 52/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.7813\n",
      "Epoch 53/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.7893\n",
      "Epoch 54/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.7984\n",
      "Epoch 55/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7859\n",
      "Epoch 56/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.7938\n",
      "Epoch 57/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.7893\n",
      "Epoch 58/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.7938\n",
      "Epoch 59/60\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.7916\n",
      "Epoch 60/60\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.7961\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Ensemble Model Accuracy: 0.8181818181818182\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "[0.12632083 0.28652292 0.27601193 0.24565091 0.66573157 0.31562383\n",
      " 0.72088737 0.21230969 0.70605165 0.15030069 0.21527284 0.38789667\n",
      " 0.95610957 0.29149618 0.8907462  0.89082341 0.22200157 0.17933256\n",
      " 0.66990883 0.32424963 0.43378983 0.48564116 0.94776241 0.39543387\n",
      " 0.84664239 0.14718098 0.96170708 0.18187706 0.56145675 0.16840985\n",
      " 0.23210006 0.1756502  0.51239684 0.56520047 0.45082275 0.1820111\n",
      " 0.73092694 0.71886368 0.16926381 0.21527284 0.11564112 0.49628383\n",
      " 0.12698261 0.91743368 0.86258486 0.16926381 0.41287847 0.21527284\n",
      " 0.92050896 0.53620962 0.47614504 0.29483969 0.85284897 0.75524681\n",
      " 0.26930802 0.10336818 0.12545653 0.16926381 0.19782127 0.96736129\n",
      " 0.1781113  0.33951388 0.17751643 0.73774131 0.69402818 0.92074132\n",
      " 0.70605165 0.40114313 0.53371765 0.78968542 0.7033299  0.17933256\n",
      " 0.7248301  0.47888355 0.96864485 0.62144154 0.21527284 0.900501\n",
      " 0.23517841 0.7033299  0.80675913 0.34439513 0.57452695 0.21527284\n",
      " 0.26930802 0.19782127 0.72688871 0.70605165 0.73092694 0.87778597\n",
      " 0.6829441  0.21527284 0.83077383 0.21527284 0.53220942 0.16926381\n",
      " 0.87090989 0.18668153 0.71413592 0.13973989 0.91070419 0.27217161\n",
      " 0.21527284 0.19174539 0.63997422 0.17338348 0.17933256 0.21527284\n",
      " 0.21527284 0.24604283 0.25099296 0.73092694 0.96736129 0.7156413\n",
      " 0.91225254 0.16303932 0.21527284 0.85253302 0.62118347 0.90770009\n",
      " 0.91939654 0.19782127 0.96214346 0.17338348 0.21527284 0.6842484\n",
      " 0.18192238 0.60281853 0.26919421 0.17017849 0.24191162 0.44650024\n",
      " 0.50903978 0.19782127 0.1229076  0.17017849 0.24542639 0.26563345\n",
      " 0.73452771 0.1030208  0.17274738 0.96364936 0.3625473  0.22837254\n",
      " 0.60029573 0.10807321 0.49628383 0.18192238 0.49628383 0.24513777\n",
      " 0.96221591 0.21527284 0.11303654 0.53620962 0.13340701 0.17017849\n",
      " 0.94307819 0.73452771 0.60029573 0.59961733 0.73092694 0.58893524\n",
      " 0.89730306 0.21527284 0.25099296 0.56996537 0.54146579 0.16685391\n",
      " 0.94776241 0.73774131 0.21527284 0.24565091 0.16099098 0.21527284\n",
      " 0.09384264 0.89162138 0.92496679 0.45680928 0.82325161 0.77613342\n",
      " 0.23517841 0.43698182 0.95300606 0.21527284 0.93544966 0.22092325\n",
      " 0.92002388 0.13887851 0.13990408 0.22092325 0.18012137 0.49628383\n",
      " 0.35569265 0.16302106 0.63567007 0.13973989 0.85934095 0.70605165\n",
      " 0.23603889 0.73092694 0.73092694 0.81443118 0.4551236  0.90173621\n",
      " 0.23000708 0.49628383 0.61486783 0.2258948  0.96364936 0.16926381\n",
      " 0.24191162 0.21527284 0.25050537 0.79026093 0.25171789 0.60029573\n",
      " 0.73092694 0.39246628 0.91221509 0.21527284 0.90753682 0.17933256\n",
      " 0.93197549 0.17933256 0.90990244 0.62709792 0.18054441 0.73092694\n",
      " 0.12736918 0.20858357 0.3690306  0.95573836 0.16643398 0.21527284\n",
      " 0.50493997 0.18237662 0.36772762 0.18237662 0.90846969 0.93841108\n",
      " 0.86725427 0.78846824 0.42086652 0.21527284 0.19972489 0.55564566\n",
      " 0.93197549 0.23461799 0.90770009 0.69782266 0.72192828 0.18237662\n",
      " 0.61630407 0.17017849 0.15900575 0.21527284 0.21527284 0.17338348\n",
      " 0.92789913 0.17933256 0.11559233 0.17933256 0.84082604 0.85253302\n",
      " 0.23517841 0.21527284 0.49628383 0.21527284 0.73092694 0.1781113\n",
      " 0.38789667 0.21527284 0.94382807 0.69782266 0.21527284 0.9127278\n",
      " 0.22837254 0.21126533 0.21713664 0.24034719 0.73452771 0.82332861\n",
      " 0.73092694 0.5449264  0.84308172 0.12197422 0.21527284 0.47614504\n",
      " 0.21527284 0.21527284 0.49628383 0.72088737 0.21527284 0.39631013\n",
      " 0.12197422 0.19174539 0.72141693 0.16840985 0.45429094 0.18668153\n",
      " 0.24191162 0.26930802 0.21644274 0.17017849 0.73092694 0.83088497\n",
      " 0.4117694  0.8263958  0.37784412 0.30282549 0.17751643 0.18192238\n",
      " 0.21527284 0.50741247 0.86725427 0.70307812 0.42048313 0.24864851\n",
      " 0.24565091 0.19008684 0.19174539 0.16926381 0.26563345 0.62173276\n",
      " 0.93605456 0.18054441 0.85491461 0.38789667 0.21158883 0.2417657\n",
      " 0.78140091 0.4795924  0.21527284 0.64906185 0.24565091 0.45429094\n",
      " 0.33951388 0.10774599 0.23000708 0.21527284 0.24263704 0.24191162\n",
      " 0.22487818 0.90916237 0.23170268 0.73466728 0.26563345 0.51916387\n",
      " 0.2258948  0.91937525 0.90872411 0.23000708 0.24263704 0.19115304\n",
      " 0.81338272 0.5474439  0.90214626 0.21527284 0.21527284 0.68734896\n",
      " 0.122844   0.89493033 0.91937525 0.24565091 0.934484   0.47212804\n",
      " 0.19782127 0.73774131 0.90872411 0.22589764 0.22794326 0.9530551\n",
      " 0.57452695 0.18315125 0.89717617 0.86026153 0.57504866 0.2417657\n",
      " 0.40996174 0.21775416 0.21527284 0.19174539 0.73092694 0.67623972\n",
      " 0.26930802 0.87152728 0.17017849 0.13423614 0.17933256 0.2070558\n",
      " 0.51082643 0.89412548 0.3876495  0.13825964 0.09639125 0.95300606\n",
      " 0.17017849 0.93145506 0.18192238 0.2134256  0.94279736 0.1599263\n",
      " 0.96170708 0.61676604 0.48997226 0.24643215 0.21572718 0.46991587\n",
      " 0.73092694 0.84529687 0.73092694 0.94070192 0.69142519 0.21527284\n",
      " 0.93605456 0.11187654 0.21527284 0.23170268]\n",
      "Test predictions saved to submission_ensemble_with_keras.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"dataset/train.csv\")\n",
    "test_data = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "# Data Preprocessing\n",
    "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]\n",
    "target = \"Survived\"\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n",
    "    df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
    "    return df[features]\n",
    "\n",
    "X_train = preprocess_data(train_data)\n",
    "y_train = train_data[target]\n",
    "X_test = preprocess_data(test_data)\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=1)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Split the resampled data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_resampled, y_train_resampled, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define a Keras neural network model\n",
    "def create_keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn=create_keras_model, epochs=60, batch_size=64, verbose=1)\n",
    "\n",
    "# Ensemble Learning (Soft Voting) including Keras NN\n",
    "models = [\n",
    "    (\"Random Forest\", RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier(n_estimators=100, random_state=1)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(criterion=\"entropy\", max_depth=4, random_state=1)),\n",
    "    (\"SVM (RBF Kernel)\", SVC(kernel=\"rbf\", C=1.0, gamma=0.2, probability=True, random_state=1)),\n",
    "    (\"Logistic Regression\", LogisticRegression(random_state=1)),\n",
    "    (\"Keras Neural Network\", keras_classifier),\n",
    "]\n",
    "\n",
    "ensemble = VotingClassifier(models, voting=\"soft\")\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model on the validation set\n",
    "ensemble_accuracy = accuracy_score(y_val, ensemble.predict(X_val))\n",
    "print(f\"Ensemble Model Accuracy: {ensemble_accuracy}\")\n",
    "\n",
    "# Get probability estimates for test data\n",
    "test_probabilities = ensemble.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply a threshold to convert probabilities to binary predictions\n",
    "print(test_probabilities)\n",
    "threshold = 0.7\n",
    "test_predictions = (test_probabilities > threshold).astype(int)\n",
    "\n",
    "# Save the test predictions to a CSV file\n",
    "output = pd.DataFrame({\"PassengerId\": test_data[\"PassengerId\"], \"Survived\": test_predictions})\n",
    "output.to_csv(\"submission_ensemble_with_keras.csv\", index=False)\n",
    "print(\"Test predictions saved to submission_ensemble_with_keras.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
